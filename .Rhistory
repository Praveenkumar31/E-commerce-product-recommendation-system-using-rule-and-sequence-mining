install.packages(tidyverse)
install.packages(c("tidyverse", "caret", "ROCR", "reshape2", "car", "corrplot", "caTools"))
install.packages(tidyverse)
install.packages("tidyverse")
subset_loan_default <- read.csv("C:/Users/91979/Desktop/FBA/Day 5/subset_loan_default.csv", header=FALSE)
View(subset_loan_default)
data = read.csv(file.path(DIR, file), header = TRUE)
data = read.csv(file.path(DIR, file), header = TRUE)
DIR = 'C:\Users\91979\Desktop\FBA\Day 5'
DIR = 'C:\Users\91979\Desktop\FBA\Day 5\'
file = 'subset_loan_default.csv'
data = read.csv(file.path(DIR, file), header = TRUE)
DIR = 'C:/Users/91979/Desktop/FBA/Day 5'
data = read.csv(file.path(DIR, file), header = TRUE)
names(data)
nrow(missing)
missing = data %>%
filter(!complete.cases(.))
missing = data %>%
filter(!complete.cases(.))
nrow(missing)
missing = data %>%
filter(!complete.cases(.))
missing = data%>%
filter(!complete.cases(.))
missing = data%>%
filter(!complete.cases(.))
install.packages("magrittr")
missing = data %>%
filter(!complete.cases(.))
missing = data %>%filter(!complete.cases(.))
missing = data %>%
filter(!complete.cases(.))
missing = data %>%
nN
missing = data %>%
filter(!complete.cases(.))
nrow(missing)
missing = data %>%
filter(!complete.cases(.))
nrow(missing)
missing = data %>%
filter(!complete.cases(.))
library(magrittr)
missing = data %>%
filter(!complete.cases(.))
nrow(missing)
nrow(missing)/nrow(data)
missing = data %>%
filter(!complete.cases(.))
nrow(missing)
nrow(missing)/nrow(data)
data = read.csv(file.path(DIR, file), header = TRUE)
names(data)
missing = data %>%
filter(!complete.cases(.))
nrow(missing)
nrow(missing)/nrow(data)
data = read.csv(file.path(DIR, file))
names(data)
missing = data %>%
filter(!complete.cases(.))
nrow(missing)
nrow(missing)/nrow(data)
DIR = 'C:/Users/91979/Desktop/FBA/Day 5'
file = 'subset_loan_default.csv'
data = read.csv(file.path(DIR, file))
names(data)
missing = data %>%
filter(!complete.cases(.))
nrow(missing)
nrow(missing)/nrow(data)
library(tidyverse)
nrow(missing)/nrow(data)
nrow(missing)
install.packages("digest")
nrow(missing)
nrow(missing)/nrow(data)
nrow(missing)/nrow(data)
nrow(missing)
subset_loan_default <- read.csv("~/subset_loan_default.csv", header=FALSE)
View(subset_loan_default)
source('~/.active-rstudio-document')
library(tidyverse)
library(corrplot)
library(car)
library(caret)
library(ROCR)
library(reshape2)
library(caTools)
library(ggplot2)
library(magrittr)
DIR = 'C:/Users/91979/Desktop/FBA/Day 5'
file = 'subset_loan_default.csv'
data = read.csv(file.path(DIR, file))
names(data)
missing = data %>%
filter(!complete.cases(.))
nrow(missing)
nrow(missing)/nrow(data)
plot(swiss)
plot(swiss)
cor(swiss)
abline(lm(mpg~wt, data = mtcars))
abline(lm(mpg~wt, data = mtcars))
plot(mtcars$wt, mtcars$mpg)
abline(lm(mpg~wt, data = mtcars))
lm(swiss)
abline(lm(education~examination, data=swiss))
abline(lm(Education~Examination, data=swiss))
abline(lm(Education~Examination, data=swiss))
plot(swiss)
abline(lm(Education~Examination, data=swiss))
abline(lm(Education~Examination, data=swiss))
plot(swiss$Education, swiss$Examination)
abline(lm(Education~Examination, data=swiss))
summary(data)
plot(swiss$Education, swiss$Examination)
plot(swiss$Education, swiss$Examination)
data <- lm(Education~Examination, data=swiss)
summary(data)
data <- lm(Fertility ~ Examination + Agriculture + Education + Catholic + Infant.Mortality, data=swiss)
summary(data)
data <- lm(Education ~ Examination, data=swiss)
summary(data)
data <- lm(Fertility ~ Examination, data=swiss)
summary(data)
library(readxl)
Car_sales <- read_excel("C:/Users/91979/Desktop/Advanced Analytics/Day 1/Dimred/Car_sales.xls")
View(Car_sales)
hist(car_sales$sales)
hist(sales)
hist(car_sales)
hist(x)
x <- Car_sales$sales
hist(x)
hist(Car_sales)
hist(Car_sales$sales)
install.packages(c("rattle", "rattle.data"))
hist(Car_sales$sales)
dataset = read.csv("C:/Users/91979/Desktop/Interviews/SAP/offline_challenge_to_send/ytrain.txt")
View(dataset)
View(dataset)
dataset = read.csv("C:/Users/91979/Desktop/Interviews/SAP/offline_challenge_to_send/ytrain.txt")
dataset$X7 = factor(dataset$X7,
levels = c('alice_in_wonderland', 'dracula', 'dubliners', 'great_expectations', 'hard_times', 'huckleberry_finn', 'les_miserable', 'moby_dick', 'oliver_twist', 'peter_pan', 'tale_of_two_cities', 'tom_sawyer'),
labels = seq(0,1,2,3,4,5,6,7,8,9,10,11))
dataset = read.csv("C:/Users/91979/Desktop/Interviews/SAP/offline_challenge_to_send/ytrain.txt")
dataset$X7 = factor(dataset$X7,
levels = c('alice_in_wonderland', 'dracula', 'dubliners', 'great_expectations', 'hard_times', 'huckleberry_finn', 'les_miserable', 'moby_dick', 'oliver_twist', 'peter_pan', 'tale_of_two_cities', 'tom_sawyer'),
labels = seq(0,11,1))
View(dataset)
dataset = read.csv("C:/Users/91979/Desktop/Interviews/SAP/offline_challenge_to_send/ytrain.txt")
dataset = factor(dataset,
levels = c('alice_in_wonderland', 'dracula', 'dubliners', 'great_expectations', 'hard_times', 'huckleberry_finn', 'les_miserable', 'moby_dick', 'oliver_twist', 'peter_pan', 'tale_of_two_cities', 'tom_sawyer'),
labels = seq(0,11,1))
view(dataset)
dataset$X7 = factor(dataset$X7,
levels = c('alice_in_wonderland', 'dracula', 'dubliners', 'great_expectations', 'hard_times', 'huckleberry_finn', 'les_miserable', 'moby_dick', 'oliver_twist', 'peter_pan', 'tale_of_two_cities', 'tom_sawyer'),
labels = seq(0,11,1))
dataset = read.csv("C:/Users/91979/Desktop/Interviews/SAP/offline_challenge_to_send/ytrain.txt")
dataset$X7 = factor(dataset$X7,
levels = c('alice_in_wonderland', 'dracula', 'dubliners', 'great_expectations', 'hard_times', 'huckleberry_finn', 'les_miserable', 'moby_dick', 'oliver_twist', 'peter_pan', 'tale_of_two_cities', 'tom_sawyer'),
labels = seq(0,11,1))
View(dataset)
View(dataset)
dataset = read.csv("C:/Users/91979/Desktop/ytrain.txt")
dataset = read.csv("C:/Users/91979/Desktop/ytrain.txt", header = TRUE)
dataset$X7 = factor(dataset$X7,
levels = c('alice_in_wonderland', 'dracula', 'dubliners', 'great_expectations', 'hard_times', 'huckleberry_finn', 'les_miserable', 'moby_dick', 'oliver_twist', 'peter_pan', 'tale_of_two_cities', 'tom_sawyer'),
labels = seq(0,11,1))
view(dataset)
dataset = read.csv("C:/Users/91979/Desktop/ytrain.txt", header = TRUE)
View(dataset)
View(dataset)
dataset = read.csv("C:/Users/91979/Desktop/ytrain.txt", header = TRUE)
View(dataset)
View(dataset)
dataset = read.csv("C:/Users/91979/Desktop/ytrain.txt", header = TRUE)
dataset = read.csv("C:/Users/91979/Desktop/ytrain.txt", header = TRUE)
dataset$i = factor(dataset$i,
levels = c('alice_in_wonderland', 'dracula', 'dubliners', 'great_expectations', 'hard_times', 'huckleberry_finn', 'les_miserable', 'moby_dick', 'oliver_twist', 'peter_pan', 'tale_of_two_cities', 'tom_sawyer'),
labels = seq(0,11,1))
dataset = read.csv("C:/Users/91979/Desktop/ytrain.txt", header = TRUE)
data = factor(dataset$i,
levels = c('alice_in_wonderland', 'dracula', 'dubliners', 'great_expectations', 'hard_times', 'huckleberry_finn', 'les_miserable', 'moby_dick', 'oliver_twist', 'peter_pan', 'tale_of_two_cities', 'tom_sawyer'),
labels = seq(0,11,1))
dataset = read.csv("C:/Users/91979/Desktop/ytrain.txt", header = TRUE)
data = factor(dataset$i,
levels = c('alice_in_wonderland', 'dracula', 'dubliners', 'great_expectations', 'hard_times', 'huckleberry_finn', 'les_miserable', 'moby_dick', 'oliver_twist', 'peter_pan', 'tale_of_two_cities', 'tom_sawyer'),
labels = seq(1,12,1))
View(dataset)
View(dataset)
write.xlsx(mydata, "C:/Users/91979/Desktop/ytrain.xlsx")
library(xlsx)
install.packages("xlsx")
library(xlsx)
write.xlsx(mydata, "C:/Users/91979/Desktop/ytrain.xlsx")
write.xlsx(dataset, "C:/Users/91979/Desktop/ytrain.xlsx")
install.packages("installr")
install.packages("arulesViz")
library(arulesViz)
library("arules");
getwd()
setwd("C:\\Users\\91979\\Desktop\\Web_analytics_V2.0")
#build rules
assoc_buy = read.transactions(file="./data/assoc_r_buy_data.csv",rm.duplicates=TRUE, format="single", sep=",", cols=c("session_id","item_id"));
assoc_buy
rules <- apriori(assoc_buy, parameter = list(supp=0.01, conf=0.2, minlen=2))
summary(rules)
inspect(rules)
# a useful plot of training data
itemFrequencyPlot(assoc_buy,topN=20,type="absolute")
#read the test data
assoc_buy = read.csv(file="./data/assoc_r_test.csv");
colnames(assoc_buy) <- c("session_id","item_id")  # set standard names, in case they are different in the data file
#execute rules against test data
makepreds <- function(itemid, rulesDF) {
antecedent = paste("{",itemid,"} =>",sep="")
firingrules = rulesDF[grep(antecedent, rulesDF$rules,fixed=TRUE),1]
gsub(" ","",toString(sub("\\}","",sub(".*=> \\{","",firingrules))))
}
rulesDF = as(rules,"data.frame")
assoc_buy$preds = apply(assoc_buy,1,function(X) makepreds(X["item_id"], rulesDF))
# extract unique predictions for each test user
#remove duplicate item_id from a basket (itemstrg)
uniqueitems <- function(itemstrg) {
unique(as.list(strsplit(gsub(" ","",itemstrg),","))[[1]])
}
userpreds = as.data.frame(aggregate(preds ~ session_id, data = assoc_buy, paste, collapse=","))
userpreds$preds = apply(userpreds,1,function(X) uniqueitems(X["preds"]))
# extract unique item_id bought (or rated highly) for each test user
baskets = as.data.frame(aggregate(item_id ~ session_id, data = assoc_buy, paste, collapse=","))
baskets$item_id = apply(baskets,1,function(X) uniqueitems(X["item_id"]))
#count how many unique predictions made are correct, i.e. have previously been bought (or rated highly) by the user
correctpreds = sum(apply(userpreds,1,function(X) checkpreds(X["preds"],X["session_id"])))
checkpreds <- function(preds, baskID) {
plist = preds[[1]]
blist = baskets[baskets$session_id == baskID,"item_id"][[1]]
cnt = 0
for (p in plist) {
if (p %in% blist) cnt = cnt+1
}
cnt
}
#count how many unique predictions made are correct, i.e. have previously been bought (or rated highly) by the user
correctpreds = sum(apply(userpreds,1,function(X) checkpreds(X["preds"],X["session_id"])))
totalpreds = sum(apply(userpreds,1,function(X) countpreds(X["preds"][[1]])))
# count all predictions made
countpreds <- function(predlist) {
len = length(predlist)
if (len > 0 && (predlist[[1]] == "")) 0 # avoid counting an empty list
else len
}
#count how many unique predictions made are correct, i.e. have previously been bought (or rated highly) by the user
correctpreds = sum(apply(userpreds,1,function(X) checkpreds(X["preds"],X["session_id"])))
totalpreds = sum(apply(userpreds,1,function(X) countpreds(X["preds"][[1]])))
# count all predictions made
countpreds <- function(predlist) {
len = length(predlist)
if (len > 0 && (predlist[[1]] == "")) 0 # avoid counting an empty list
else len
}
precision = correctpreds*100/totalpreds
precision = correctpreds*100/totalpreds
cat("precision=", precision, "corr=",correctpreds,"total=",totalpreds)
library(arulesViz)
install.packages("rlang")
library(arulesViz)
install.packages("rlang")
library(arulesViz)
plot(rules)
library("arules");
getwd()
setwd("C:\\Users\\91979\\Desktop\\Web_analytics_V2.0")
#build rules
assoc_buy = read.transactions(file="./data/assoc_r_buy_data.csv",rm.duplicates=TRUE, format="single", sep=",", cols=c("session_id","item_id"));
assoc_buy
rules <- apriori(assoc_buy, parameter = list(supp=0.01, conf=0.2, minlen=2))
summary(rules)
inspect(rules)
# a useful plot of training data
itemFrequencyPlot(assoc_buy,topN=20,type="absolute")
#read the test data
assoc_buy = read.csv(file="./data/assoc_r_test.csv");
install.packages("arules")
install.packages("arules")
install.packages("arules")
install.packages("arules")
library("arules");
getwd()
setwd("C:\\Users\\91979\\Desktop\\Web_analytics_V2.0")
#build rules
assoc_buy = read.transactions(file="./data/assoc_r_buy_data.csv",rm.duplicates=TRUE, format="single", sep=",", cols=c("session_id","item_id"));
assoc_buy
rules <- apriori(assoc_buy, parameter = list(supp=0.01, conf=0.2, minlen=2))
summary(rules)
inspect(rules)
# a useful plot of training data
itemFrequencyPlot(assoc_buy,topN=20,type="absolute")
#read the test data
assoc_buy = read.csv(file="./data/assoc_r_test.csv");
colnames(assoc_buy) <- c("session_id","item_id")  # set standard names, in case they are different in the data file
#execute rules against test data
makepreds <- function(itemid, rulesDF) {
antecedent = paste("{",itemid,"} =>",sep="")
firingrules = rulesDF[grep(antecedent, rulesDF$rules,fixed=TRUE),1]
gsub(" ","",toString(sub("\\}","",sub(".*=> \\{","",firingrules))))
}
rulesDF = as(rules,"data.frame")
assoc_buy$preds = apply(assoc_buy,1,function(X) makepreds(X["item_id"], rulesDF))
# extract unique predictions for each test user
#remove duplicate item_id from a basket (itemstrg)
uniqueitems <- function(itemstrg) {
unique(as.list(strsplit(gsub(" ","",itemstrg),","))[[1]])
}
userpreds = as.data.frame(aggregate(preds ~ session_id, data = assoc_buy, paste, collapse=","))
userpreds$preds = apply(userpreds,1,function(X) uniqueitems(X["preds"]))
